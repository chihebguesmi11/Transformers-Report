# Transformers-Report
# From Attention to Instruction: Diving into Transformer Models and Prompt Design

[![Transformers](https://img.shields.io/badge/Transformers-HuggingFace-blue)](https://huggingface.co/transformers/)
[![NLP](https://img.shields.io/badge/NLP-Natural%20Language%20Processing-red)]()
[![LLMs](https://img.shields.io/badge/LLMs-Large%20Language%20Models-green)]()
[![Academic](https://img.shields.io/badge/Type-Academic_Report-lightgrey)]()
[![License](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-yellow)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

---

##  Overview

This report explores the rise of Transformer models and Large Language Models (LLMs), aiming to explain their inner workings, architectures, and practical usage through prompt design.

It was written as part of my Master's coursework in Artificial Intelligence & Data Science at **Université Paris Dauphine | Campus de Tunis**, academic year **2024/2025**.

---

## Table of Contents

- [Introduction](#introduction)
- [1. Foundations of Transformer Models](#1-foundations-of-transformer-models)
- [2. Transformer Architecture](#2-transformer-architecture)
- [3. Introduction to LLMs](#3-introduction-to-llms)
- [4. Prompt Engineering Techniques](#4-prompt-engineering-techniques)
- [Conclusion](#conclusion)
- [License](#license)
- [Credits](#credits)

---

## Introduction

Transformers have become the cornerstone of modern NLP and LLM development. However, the architectural and practical insights behind their capabilities are often elusive. This report attempts to bridge that gap by offering:

- A historical context of NLP evolution
- A deep dive into attention mechanisms and core transformer components
- A classification of transformer architectures (BERT, GPT, T5)
- An introduction to LLM inference and decoding techniques
- A practical guide to prompt engineering for real-world applications

---

## 1. Foundations of Transformer Models

Covers the timeline of NLP leading to transformers and demonstrates Hugging Face pipelines such as:
- Zero-shot classification
- Text generation
- Fill-mask prediction

---

## 2. Transformer Architecture

Explains the core mechanics of transformers:
- Self-attention & multi-head attention
- Positional encoding
- Encoder vs decoder stacks
- Residual connections and LayerNorm
- Model types: encoder-only (BERT), decoder-only (GPT), and encoder-decoder (T5)

---

## 3. Introduction to LLMs

Defines large language models and how they function:
- Pretraining vs inference
- Prefill and decode phases
- Sampling strategies (temperature, top-k, top-p)
- Model limitations and ethical considerations

---

## 4. Prompt Engineering Techniques

Breaks down effective prompting strategies:
- Zero-shot / Few-shot prompting
- Chain-of-thought prompting
- Role prompting and output formatting
- Examples and prompt optimization insights (e.g., PromptPerfect)

---

## Conclusion

This work summarizes foundational concepts in transformers and LLMs with a clear pedagogical approach. It is designed as a reference for learners, developers, and researchers who wish to understand and apply state-of-the-art NLP architectures.

---

## License

This work is shared under the **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)** License.  
You are free to use, share, and adapt it **for non-commercial purposes**, as long as you:
- Give appropriate credit
- Share any derivative works under the same license

[Read License Details Here](https://creativecommons.org/licenses/by-nc-sa/4.0/)

---

## Credits

- Author: **Chiheb GUESMI**
- University: **Université Paris Dauphine | Campus de Tunis**
- Academic Year: **2024/2025**
- Figures and references from:
  - Hugging Face documentation
  - Jalammar's visual explanations
  - Official Transformer paper: "Attention Is All You Need"

